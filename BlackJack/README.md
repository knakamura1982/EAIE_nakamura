## 基本的な設計思想
- ディーラープログラムとプレイヤープログラムが分かれており，ソケット通信をしながらゲームを進めます．
  - 様々な戦略のプレイヤーが同一のディーラーを相手に戦っている，ような雰囲気を出すための設計です．
- ディーラーにしか把握できない（はずの）情報は全てディーラープログラム側（dealer.py）に記載されています．
  - ですので，config.py や classes.py に記載の情報はプレイヤープログラム側から参照してOKです．
  - 逆に dealer.py に記載の情報はプレイヤープログラム側からは参照しない，というルールです．
- ディーラープログラムを相手に人間がプレイすることも可能です．

## 実行手順
- 前提として，ターミナルが複数必要になります．
- まず，一方のターミナルで dealer.py を実行します
- dealer.py が接続待ち状態になったら，もう一方のターミナルでプレイヤープログラムを何か一つ実行します．
- プレイヤープログラムが終了しても，dealer.py は終了せず接続待ち状態に入りますので，  
以降はプレイヤープログラムを実行するだけでOKです．
- dealer.py を終了したい場合は Ctrl+C で止めて下さい．

## dealer.py

ディーラープログラム．基本的に実行するだけです．
**コマンド**
```
python dealer.py
```
**オプション**
特にありません．

## human_player.py

ディーラープログラムを相手に人間がプレイする際に使用するプログラム．GUIで操作できます．  
**コマンド**
```
python human_player.py --history play_log.csv
```
**オプション**
- history
  - プレイヤーが取った行動をログとして記録する際の記録先ファイル．
    - このログファイルをニューラルネットワークの学習データとして使用することも可能です．
    - また，348～360行目の get_state 関数を改造し，さらに384行目と395行目を適切に修正すれば，  
    様々な情報を記録できるようになります．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．

## ai_player_rand.py

## ai_player_NN.py

## ai_player_Q.py

## log_selector.py

プレイヤープログラム実行時に出力されたログファイルから「都合が良い」行のみを抽出するプログラム．  
特定条件下のログのみ（例えば勝った時のログのみ，など）を用いて  
ニューラルネットワークを学習したい場合に使用する想定で用意しました．  
なお，何を以て「都合が良い」とするかはプレイヤーAIの実装方針に沿って各自で策定して頂き，  
その内容に応じて16行目の内容をご変更ください（13～15行目に記載のコメントも参照）．  

**コマンド**
```
python log_selector.py --in_file play_log.csv --out_file selected_log.csv
```
**オプション**
- in_file
  - プレイヤープログラムから出力された生のログファイル．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．
- out_file
  - 抽出結果の保存先ファイル．
  - 指定しなかった場合，デフォルト値として selected_log.csv がセットされます．

## NN_train.py

AIプレイヤーの行動選択用ニューラルネットワークを学習するプログラム．  
プレイヤープログラムから出力された生のログファイル，  
もしくは log_selector.py で抽出したログファイルを学習データとして使用する想定です．  
学習データとして読み込むファイルは17行目で指定します．  

**コマンド**
```
python NN_train.py --gpu 0 --epochs 50 --batchsize 100 --model BJNet_model.pth --autosave
```

**オプション**
- gpu
  - 使用するGPUのID (-1を指定するとCPU上で動作します)
  - このオプションを指定しない場合，デフォルト値として -1 がセットされます．
  - cudaを使用できない環境では無視されます．
- epochs
  - 何エポック分学習するか
  - このオプションを指定しない場合，デフォルト値として 50 がセットされます．
- batchsize
  - バッチサイズ
  - このオプションを指定しない場合，デフォルト値として 100 がセットされます．
- model
  - 学習結果のモデルパラメータの保存先
  - このオプションを指定しない場合，デフォルト値として ./BJNet_models/model.pth がセットされます．
- autosave
  - 指定すると毎エポック終了時にモデルパラメータが自動保存されるようになります．
  - 保存先は ./BJNet_models/autosaved_model_epX.pth です（ X はエポック番号 ）．

## QTable_checker.py

## BJNet_models

NN_train.py による学習結果の保存先として使用する想定のフォルダ．  
動作テスト時に作成したファイルが model.pth という名前で残っているが，決して良いモデルではありません．

## imgs

トランプカードの図柄画像が収められています（human_player.py でのみ使用）．  
なお，素材は下記サイトからお借りしました．  
https://chicodeza.com/freeitems/torannpu-illust.html

## その他

以下のソースファイルは実行対象ではありません．
- **NN_structure.py**
  - ai_player_NN.py で使用するニューラルネットワークの構造が記載されているファイル．
- **classes.py**
  - カードの配布とシャッフル，手札の管理，ディーラーとプレイヤーの通信処理など，  
  雑多な処理を担当するクラス群が記載されているファイル．
- **config.py**
  - 使用するカードデッキの数，カードシャッフルの頻度，ソケット通信のポート番号，  
  といった各種設定値が記載されているファイル．  
  これらの設定値は，コンペティションの際を除き，自由に変更して頂いて結構です．  
  コンペティション時は元の値を使用する予定ですが，希望に応じて変更も検討致します．
