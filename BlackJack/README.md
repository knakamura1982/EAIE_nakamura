## 基本的な設計思想
- ディーラープログラムとプレイヤープログラムに分かれており，両者がソケット通信をしながらゲームを進めます．
  - 様々な戦略のプレイヤーが同一のディーラーを相手に戦っている，ような雰囲気を出すための設計です．
- ディーラーにしか把握できない（はずの）情報は全てディーラープログラム側（dealer.py）に記載されています．
  - ですので，config.py や classes.py に記載の情報はプレイヤープログラム側から参照してOKです．
  - 逆に dealer.py に記載の情報はプレイヤープログラム側からは参照しない，というルールです．
- ディーラープログラムを相手に人間がプレイすることも可能です．

## 実行手順
- 前提として，ターミナルが複数必要になります．
- まず，一方のターミナルで dealer.py を実行します
- dealer.py が接続待ち状態になったら，もう一方のターミナルでプレイヤープログラムを何か一つ実行します．
- プレイヤープログラムが終了しても，dealer.py は終了せず接続待ち状態に入りますので，  
以降はプレイヤープログラムを実行するだけでOKです．
- dealer.py を終了したい場合は Ctrl+C で止めて下さい．

## ソケット通信に失敗する場合の対処
- ディーラー・プレイヤー間の通信に失敗する場合は，以下を試してみて下さい（これで直る保証はありませんが，多少は改善されるかもしれません）．
  - dealer.py における ```soc.bind((socket.gethostname(), PORT))``` の部分を ```soc.bind(('127.0.0.1', PORT))``` に変更
  - 各種プレイヤープログラムにおける ```soc.connect((socket.gethostname(), PORT))``` の部分を ```soc.connect(('127.0.0.1', PORT))``` に変更

## dealer.py

ディーラープログラム．基本的に実行するだけです．  
終了したい場合は Ctrl+C で止めて下さい．

**コマンド例**
```
python dealer.py
```

**オプション**

特にありません．

## human_player.py

ディーラープログラムを相手に人間がプレイする際に使用するプログラム．GUIで操作できます．

**コマンド例**
```
python human_player.py --history play_log.csv
```

**オプション**
- history
  - プレイヤーが取った行動をログとして記録する際の記録先ファイル．
    - このログファイルをニューラルネットワークの学習データとして使用することも可能です．
    - また，409～423行目の get_state 関数を改造し，さらに445行目と456行目の print 文を適切に修正すれば，  
    様々な情報を記録できるようになります．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．

## ai_player_rand.py

ランダム戦略（現在の状態に関係なくランダムに行動を選択する戦略）で動作するAIプレイヤーのプログラム．  

**コマンド例**
```
python ai_player_rand.py --games 10 --history play_log.csv
```

**オプション**
- games
  - 連続して何回ゲームをプレイするか
  - 指定しなかった場合，デフォルト値として 1 がセットされます（つまり1回だけプレイして終了）．
- history
  - AIプレイヤーが選択した行動をログとして記録する際の記録先ファイル．
    - 225～239行目の get_state 関数を改造し，さらに271行目と302行目の print 文を適切に修正すれば，  
    様々な情報を記録できるようになります．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．

## ai_player_NN.py

ニューラルネットワークに基づいて行動を選択するAIプレイヤーのプログラム．  

**コマンド例**
```
python ai_player_NN.py --games 10 --history play_log.csv --gpu 0 --model BJNet_model.pth
```

**オプション**
- games
  - 連続して何回ゲームをプレイするか
  - 指定しなかった場合，デフォルト値として 1 がセットされます（つまり1回だけプレイして終了）．
- history
  - AIプレイヤーが選択した行動をログとして記録する際の記録先ファイル．
    - 243～257行目の get_state 関数を改造し，さらに290行目と327行目の print 文を適切に修正すれば，  
    様々な情報を記録できるようになります．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．
- gpu
  - 使用するGPUのID (-1を指定するとCPU上で動作します)
  - このオプションを指定しない場合，デフォルト値として -1 がセットされます．
  - cudaを使用できない環境では無視されます．
- model
  - 使用するニューラルネットワークのモデルパラメータファイル
  - このオプションを指定しない場合，デフォルト値として ./BJNet_models/model.pth が読み込まれます．

## ai_player_Q.py

Q学習を用いてリアルタイムに戦略（行動選択基準）を獲得するAIプレイヤーのプログラム．  

**コマンド例**
```
# 一から学習する場合
python ai_player_Q.py --games 10 --history play_log.csv --save QTable.pkl

# 学習済みQテーブルをロードし，そこから学習を再開する場合
python ai_player_Q.py --games 10 --history play_log.csv --load QTable.pkl --save new_QTable.pkl

# 学習済みQテーブルをロードして単にゲームをプレイするだけの場合
python ai_player_Q.py --games 10 --history play_log.csv --load QTable.pkl --testmode
```

**オプション**
- games
  - 連続して何回ゲームをプレイするか
  - 指定しなかった場合，デフォルト値として 1 がセットされます（つまり1回だけプレイして終了）．
- history
  - AIプレイヤーが選択した行動をログとして記録する際の記録先ファイル．
    - 233～247行目の get_state 関数を改造し，さらに297行目と338行目の print 文を適切に修正すれば，  
    様々な情報を記録できるようになります．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．
- load
  - 指定したファイルからQテーブルをロードします．
  - 必ずしも指定する必要はありません．指定しない場合，Qテーブルは全てのフィールドが 0 で初期化されます．
- save
  - 指定したファイルに学習結果のQテーブルが保存されます（既に存在するファイル名を指定した場合は上書きされます）．
  - 必ずしも指定する必要はありません．指定しない場合，学習結果は保存されません（プログラム終了時に破棄されます）．
- testmode
  - 指定すると，常にQ値最大の行動を選択するようになります（ε-greedy における ε=0 の状態）．
  - このモードで動作しているときはQテーブルは更新されません．

## log_selector.py

プレイヤープログラム実行時に出力されたログファイルから「都合が良い」行のみを抽出するプログラム（のひな型）．  
特定条件下のログのみ（例えば勝った時のログのみ，など）を用いて  
ニューラルネットワークを学習したい場合に使用する想定で用意しました．  
なお，何をもって「都合が良い」とするかはプレイヤーAIの実装方針に沿って各自で策定して頂き，  
その内容に応じて16行目の内容をご変更ください（元の記載のままではロクなAIプレイヤーになりません）．  
16行目そのものを削除し，独自の方針でプログラムを改造して頂いても問題ございません．  
また，ログファイル解析・抽出用の独自プログラムを用意して頂くのもOKです．  

**コマンド例**
```
python log_selector.py --in_file play_log.csv --out_file selected_log.csv
```
**オプション**
- in_file
  - プレイヤープログラムから出力された生のログファイル．
  - 指定しなかった場合，デフォルト値として play_log.csv がセットされます．
- out_file
  - 抽出結果の保存先ファイル．
  - 指定しなかった場合，デフォルト値として selected_log.csv がセットされます．

## NN_train.py

AIプレイヤーの行動選択用ニューラルネットワークを学習するプログラム．  
プレイヤープログラムから出力された生のログファイル，  
もしくは log_selector.py で抽出したログファイルを学習データとして使用する想定です．  
学習データとして読み込むファイルは17行目で指定します．  

**コマンド例**
```
python NN_train.py --gpu 0 --epochs 50 --batchsize 100 --model BJNet_model.pth --autosave
```

**オプション**
- gpu
  - 使用するGPUのID (-1を指定するとCPU上で動作します)
  - このオプションを指定しない場合，デフォルト値として -1 がセットされます．
  - cudaを使用できない環境では無視されます．
- epochs
  - 何エポック分学習するか
  - このオプションを指定しない場合，デフォルト値として 50 がセットされます．
- batchsize
  - バッチサイズ
  - このオプションを指定しない場合，デフォルト値として 100 がセットされます．
- model
  - 学習結果のモデルパラメータの保存先
  - このオプションを指定しない場合，デフォルト値として ./BJNet_models/model.pth がセットされます．
- autosave
  - 指定すると毎エポック終了時にモデルパラメータが自動保存されるようになります．
  - 保存先は ./BJNet_models/autosaved_model_epX.pth です（ X はエポック番号 ）．

## QTable_checker.py

保存済みのQテーブル（ai_player_Q.py で学習したもの）の内容を出力するプログラム．

**コマンド例**
```
python QTable_checker.py --file QTable.pkl

# 以下のようにして csv ファイルに書き出してから Excel で確認した方が分かりやすいかも
python QTable_checker.py --file QTable.pkl > QTable.csv
```

**オプション**
- file
  - ai_player_Q.py の実行時に保存したQテーブルファイル．

## BJNet_models

NN_train.py による学習結果の保存先として使用する想定のフォルダ．  
動作テスト時に作成したファイルが model.pth という名前で残っていますが，決して良いモデルではありません．

## imgs

トランプカードの図柄画像が収められています（human_player.py でのみ使用）．  
なお，素材は下記サイトから拝借しました．  
https://chicodeza.com/freeitems/torannpu-illust.html

## その他

以下のソースファイルは実行対象ではありません．
- **NN_structure.py**
  - ai_player_NN.py で使用するニューラルネットワークの構造が記載されているファイル．
  - ニューラルネットワークを改造する際には，このファイルをご修正ください．
- **classes.py**
  - カードの配布とシャッフル，手札の管理，ディーラーとプレイヤーの通信処理など，  
  雑多な処理を担当するクラス群が記載されているファイル．
- **config.py**
  - 使用するカードデッキの数，カードシャッフルの頻度，ソケット通信のポート番号，  
  といった各種設定値が記載されているファイル．  
  これらの設定値は，コンペティションの際を除き，自由に変更して頂いて結構です．  
  コンペティション時は元の値を使用する予定ですが，希望に応じて変更も検討致します．
